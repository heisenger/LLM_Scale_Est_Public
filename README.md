# Magnitude Estimation in LLMs

This repository provides a framework for benchmarking large language models (LLMs) on scale and magnitude estimation tasks.

## Repository Structure

- `analysis/` — Data analysis, model fitting, and graph generation
  - `thesis_graphs/` — Scripts for creating publication-quality graphs
- `configs/experiments/` — Experiment configuration files and settings
  - `config.py` — Main settings for experiments
  - `*.yaml` — YAML files specifying which experiments to run
- `experiment_runners/` — Experiment execution logic
- `models/` — Model metadata, prompt formatting, and API key storage
  - `env/api.env` — **Required:** Save your API key here
- `notebooks/` — Jupyter notebooks for analysis and visualization
- `outputs/` — Results and generated figures
- `script/` — Main scripts for running experiments and plots
- `utils/` — Utility functions for data and model processing

Refer to `schema.md` for a detailed description of each directory and file.

## Instructions

1. **Set up your API key:**
   - Save your API key in `models/env/api.env` (required for LLM access).

2. **Configure your experiment:**
   - Edit settings in `configs/experiments/config.py` as needed.
   - Select or create a YAML file in `configs/experiments/` (e.g., `exp_run.yaml`) to specify the experiment.

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Run an experiment:**
   ```bash
   python -m script.run_main --config configs/experiments/exp_run.yaml
   ```
   - Replace `exp_run.yaml` with the YAML file for your experiment of interest.

5. **View results and graphs:**
   - Output data and figures are saved in `outputs/`.
   - Publication-quality graphs are generated by scripts in `analysis/thesis_graphs/`.

## Notes
- All experiment settings can be manipulated in `configs/experiments/config.py`.
- Make sure your API key is valid and saved before running any experiments.
- For more details, see `schema.md` and comments in the configuration files.
